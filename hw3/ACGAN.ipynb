{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1184bbd3-c9b7-43ff-8620-738893d40885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f73f75e1-2765-48ae-a3d1-d4d6d8c40e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device setup\n",
    "device = 'cpu'\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c35e09d4-c304-4a72-8126-bd6b03d114ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#set up seed for consistancy\n",
    "torch.manual_seed(1)\n",
    "#hyperparameters based on slides\n",
    "learning_rate = 2e-4\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "#black and white 1 for color img 3(r.g.b)\n",
    "image_channels = 3\n",
    "noise_dimension = 100\n",
    "epochs = 1\n",
    "discriminator_features = 64\n",
    "generator_features = 64\n",
    "adam_beta = 0.5\n",
    "latent_dim = 100\n",
    "n_classes = 10\n",
    "#get Cifar10 dataset\n",
    "workers = 2\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(image_channels)], [0.5 for _ in range(image_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "data_train = torchvision.datasets.CIFAR10(root='./DL HW 3/data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms,  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                          shuffle=True,num_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6918129b-8cc7-4c17-b1b6-f0f76817add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discrim = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(3, features_d, 4, 2, 1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._descrimStructure(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._descrimStructure(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._descrimStructure(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            \n",
    "        )\n",
    "       \n",
    "        self.adv_layer = nn.Sequential(\n",
    "                                nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=1, padding=0),\n",
    "                                nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(\n",
    "                                nn.Conv2d(features_d * 8, 10, 4, 1, 0, bias = False), \n",
    "                                nn.LogSoftmax(dim = 1))\n",
    "\n",
    "    def _descrimStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.discrim(x)\n",
    "        adv = self.adv_layer(x).view(-1)\n",
    "        aux = self.aux_layer(x).view(-1,10)\n",
    "        return adv,aux\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = image_size   # Initial size before upsampling\n",
    "        #self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "        self.genet = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim+100,features_g* 16, 4, 1, 0, bias = False),\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._genetStructure(features_g* 16, features_g* 8, 4, 2, 1),  \n",
    "            self._genetStructure(features_g* 8, features_g* 4, 4, 2, 1), \n",
    "            self._genetStructure(features_g* 4, features_g* 2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g* 2, 3, 4, 2, 1),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embedding_layer = nn.Embedding(10, 100)\n",
    "\n",
    "    def _genetStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels,0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        embed = self.embedding_layer(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([noise, embed], dim=1)\n",
    "        return self.genet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "004eca9e-e422-4aeb-a516-84737d46c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c34f9184-d401-4b26-9e24-58aa7a9a687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (genet): Sequential(\n",
      "    (0): ConvTranspose2d(200, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (embedding_layer): Embedding(10, 100)\n",
      ")\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 0/782                   Loss D: 0.6943, loss G: 0.6863\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 1/782                   Loss D: 0.6704, loss G: 0.6729\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 2/782                   Loss D: 0.6345, loss G: 0.6499\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 3/782                   Loss D: 0.5804, loss G: 0.6431\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 4/782                   Loss D: 0.4943, loss G: 0.7189\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 5/782                   Loss D: 0.4030, loss G: 0.9028\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 6/782                   Loss D: 0.3334, loss G: 1.1700\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 7/782                   Loss D: 0.2367, loss G: 1.6576\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 8/782                   Loss D: 0.1321, loss G: 2.6014\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 9/782                   Loss D: 0.1029, loss G: 4.0633\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 10/782                   Loss D: 0.1013, loss G: 2.8370\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 11/782                   Loss D: 0.5882, loss G: 5.4768\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 12/782                   Loss D: 0.0753, loss G: 5.2173\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 13/782                   Loss D: 0.0992, loss G: 3.9144\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 14/782                   Loss D: 0.0706, loss G: 3.5227\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 15/782                   Loss D: 0.1340, loss G: 7.1983\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 16/782                   Loss D: 0.0991, loss G: 6.9872\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 17/782                   Loss D: 0.0244, loss G: 5.3605\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 18/782                   Loss D: 0.0136, loss G: 4.9662\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 19/782                   Loss D: 0.0181, loss G: 4.8680\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 20/782                   Loss D: 0.0248, loss G: 6.1909\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 21/782                   Loss D: 0.0272, loss G: 7.1175\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 22/782                   Loss D: 0.0075, loss G: 7.8639\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 23/782                   Loss D: 0.0090, loss G: 7.2893\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 24/782                   Loss D: 0.0051, loss G: 7.0065\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 25/782                   Loss D: 0.0068, loss G: 7.1727\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 26/782                   Loss D: 0.0067, loss G: 6.7950\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 27/782                   Loss D: 0.0070, loss G: 6.4176\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 28/782                   Loss D: 0.0105, loss G: 7.2594\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 29/782                   Loss D: 0.0056, loss G: 7.9346\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 30/782                   Loss D: 0.0067, loss G: 8.5751\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 31/782                   Loss D: 0.0046, loss G: 8.1944\n",
      "torch.Size([64, 100, 1, 1])\n",
      "Epoch [0/1] Batch 32/782                   Loss D: 0.0019, loss G: 7.9138\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:73] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m disc_real,disc_label \u001b[38;5;241m=\u001b[39m discrim(real)\n\u001b[1;32m     38\u001b[0m loss_disc_real \u001b[38;5;241m=\u001b[39m criterion(disc_real\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mones_like(disc_real))\n\u001b[0;32m---> 39\u001b[0m disc_fake,disc_label_fk \u001b[38;5;241m=\u001b[39m \u001b[43mdiscrim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss_disc_fake \u001b[38;5;241m=\u001b[39m criterion(disc_fake\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mzeros_like(disc_fake))\n\u001b[1;32m     41\u001b[0m loss_disc \u001b[38;5;241m=\u001b[39m (loss_disc_real \u001b[38;5;241m+\u001b[39m loss_disc_fake) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscrim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madv_layer(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_layer(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:73] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 268435456 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "genet = Generator(noise_dimension, image_channels, generator_features).to(device)\n",
    "discrim = Discriminator(image_channels, discriminator_features).to(device)\n",
    "#genet.summary()\n",
    "#discrim.summary()\n",
    "print(genet)\n",
    "model_weights(genet)\n",
    "model_weights(discrim)\n",
    "gen_loss = []\n",
    "disc_loss = []\n",
    "opt_gen = optim.Adam(genet.parameters(), lr=learning_rate, betas=(0.5,0.999))\n",
    "opt_disc = optim.Adam(discrim.parameters(), lr=learning_rate, betas=(0.5,0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(10, noise_dimension, 1, 1).to(device)\n",
    "sample_label = torch.randint(0,10,(10,),dtype = torch.long).to(device)\n",
    "step = 0\n",
    "\n",
    "genet.train()\n",
    "discrim.train()\n",
    "fakeimg_list=[]\n",
    "img_list=[]\n",
    "for epoch in range(epochs):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(train_loader):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(batch_size, noise_dimension, 1, 1).to(device)\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, 64))).to(device)\n",
    "      \n",
    "        print(noise.shape)\n",
    "        fake = genet(noise,gen_labels)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real,disc_label = discrim(real)\n",
    "        loss_disc_real = criterion(disc_real.reshape(-1), torch.ones_like(disc_real))\n",
    "        disc_fake,disc_label_fk = discrim(fake.detach())\n",
    "        loss_disc_fake = criterion(disc_fake.reshape(-1), torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        discrim.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output,lbel = discrim(fake)\n",
    "        loss_gen = criterion(output.reshape(-1), torch.ones_like(output))\n",
    "        genet.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        gen_loss.append(loss_gen)\n",
    "        disc_loss.append(loss_disc)\n",
    "        \n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if 0 % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fak = genet(fixed_noise,sample_label).detach().cpu()\n",
    "                realI = real.detach().cpu()\n",
    "                fakeimg_list.append(torchvision.utils.make_grid(fak, padding=2, normalize=True))\n",
    "                img_list.append(torchvision.utils.make_grid(realI, padding=2, normalize=True))\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca3777-9f22-4f0b-8c50-3965f63eac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a59101-04d7-4a19-8736-7de500bbc732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
