{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b93121-41b1-4d6a-96ec-16103d8adf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc51dbb-9ed8-40a9-9d99-2cb6fb9fb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d7d0a5-00cd-4e0b-aaed-954734914d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#set up seed for consistancy\n",
    "torch.manual_seed(1)\n",
    "#hyperparameters\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "#black and white 1 for color img 3(r.g.b)\n",
    "image_channels = 3\n",
    "z_dimension = 128\n",
    "epochs = 25\n",
    "discriminator_features = 64\n",
    "generator_features = 64\n",
    "discriminator_iterations = 5\n",
    "weight = 0.01\n",
    "#get Cifar10 dataset\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(image_channels)], [0.5 for _ in range(image_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "data_train = torchvision.datasets.CIFAR10(root='./DL HW 3/data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms,  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f8a5d6-8526-4f09-aaad-351f2c35be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discrim = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._descrimStructure(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._descrimStructure(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._descrimStructure(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _descrimStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discrim(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.genet = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._genetStructure(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._genetStructure(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._genetStructure(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._genetStructure(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _genetStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.genet(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e19a1-dcfe-4764-a172-4a4fabbdd0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 100/782                   Loss D: -1.3417, loss G: 0.6535\n",
      "Epoch [0/50] Batch 200/782                   Loss D: -1.5074, loss G: 0.7373\n",
      "Epoch [0/50] Batch 300/782                   Loss D: -1.4969, loss G: 0.7334\n",
      "Epoch [0/50] Batch 400/782                   Loss D: -1.4941, loss G: 0.7283\n",
      "Epoch [0/50] Batch 500/782                   Loss D: -1.4632, loss G: 0.7283\n",
      "Epoch [0/50] Batch 600/782                   Loss D: -1.4718, loss G: 0.7305\n",
      "Epoch [0/50] Batch 700/782                   Loss D: -1.4431, loss G: 0.7056\n",
      "Epoch [1/50] Batch 100/782                   Loss D: -1.2528, loss G: 0.6522\n",
      "Epoch [1/50] Batch 200/782                   Loss D: -1.2607, loss G: 0.6701\n",
      "Epoch [1/50] Batch 300/782                   Loss D: -1.1206, loss G: 0.6654\n",
      "Epoch [1/50] Batch 400/782                   Loss D: -1.1825, loss G: 0.6197\n",
      "Epoch [1/50] Batch 500/782                   Loss D: -1.1205, loss G: 0.6025\n",
      "Epoch [1/50] Batch 600/782                   Loss D: -1.0761, loss G: 0.5693\n",
      "Epoch [1/50] Batch 700/782                   Loss D: -0.9929, loss G: 0.4076\n",
      "Epoch [2/50] Batch 100/782                   Loss D: -0.9879, loss G: 0.6201\n",
      "Epoch [2/50] Batch 200/782                   Loss D: -1.0595, loss G: 0.5843\n",
      "Epoch [2/50] Batch 300/782                   Loss D: -0.8782, loss G: 0.3606\n",
      "Epoch [2/50] Batch 400/782                   Loss D: -0.8648, loss G: 0.6169\n",
      "Epoch [2/50] Batch 500/782                   Loss D: -0.8965, loss G: 0.3635\n",
      "Epoch [2/50] Batch 600/782                   Loss D: -0.8961, loss G: 0.3726\n",
      "Epoch [2/50] Batch 700/782                   Loss D: -0.9880, loss G: 0.5531\n",
      "Epoch [3/50] Batch 100/782                   Loss D: -1.0170, loss G: 0.5874\n",
      "Epoch [3/50] Batch 200/782                   Loss D: -0.8840, loss G: 0.3024\n",
      "Epoch [3/50] Batch 300/782                   Loss D: -0.9501, loss G: 0.4164\n",
      "Epoch [3/50] Batch 400/782                   Loss D: -0.8247, loss G: 0.5988\n",
      "Epoch [3/50] Batch 500/782                   Loss D: -0.9469, loss G: 0.6021\n",
      "Epoch [3/50] Batch 600/782                   Loss D: -0.8632, loss G: 0.5909\n",
      "Epoch [3/50] Batch 700/782                   Loss D: -0.9704, loss G: 0.3928\n",
      "Epoch [4/50] Batch 100/782                   Loss D: -0.9143, loss G: 0.6127\n",
      "Epoch [4/50] Batch 200/782                   Loss D: -0.8534, loss G: 0.2862\n",
      "Epoch [4/50] Batch 300/782                   Loss D: -0.9171, loss G: 0.5821\n",
      "Epoch [4/50] Batch 400/782                   Loss D: -0.9333, loss G: 0.3949\n",
      "Epoch [4/50] Batch 500/782                   Loss D: -0.8239, loss G: 0.2835\n",
      "Epoch [4/50] Batch 600/782                   Loss D: -0.8955, loss G: 0.3917\n",
      "Epoch [4/50] Batch 700/782                   Loss D: -0.8182, loss G: 0.6053\n",
      "Epoch [5/50] Batch 100/782                   Loss D: -0.8204, loss G: 0.5963\n",
      "Epoch [5/50] Batch 200/782                   Loss D: -0.8161, loss G: 0.5830\n",
      "Epoch [5/50] Batch 300/782                   Loss D: -0.8923, loss G: 0.3333\n",
      "Epoch [5/50] Batch 400/782                   Loss D: -0.8599, loss G: 0.3828\n",
      "Epoch [5/50] Batch 500/782                   Loss D: -0.8753, loss G: 0.5669\n",
      "Epoch [5/50] Batch 600/782                   Loss D: -0.8493, loss G: 0.5912\n",
      "Epoch [5/50] Batch 700/782                   Loss D: -0.8322, loss G: 0.4750\n",
      "Epoch [6/50] Batch 100/782                   Loss D: -0.7327, loss G: 0.1521\n",
      "Epoch [6/50] Batch 200/782                   Loss D: -0.8497, loss G: 0.3163\n",
      "Epoch [6/50] Batch 300/782                   Loss D: -0.8201, loss G: 0.5802\n",
      "Epoch [6/50] Batch 400/782                   Loss D: -0.8953, loss G: 0.5971\n",
      "Epoch [6/50] Batch 500/782                   Loss D: -0.7624, loss G: 0.2427\n",
      "Epoch [6/50] Batch 600/782                   Loss D: -0.8515, loss G: 0.2702\n",
      "Epoch [6/50] Batch 700/782                   Loss D: -0.8785, loss G: 0.5655\n",
      "Epoch [7/50] Batch 100/782                   Loss D: -0.8085, loss G: 0.3940\n",
      "Epoch [7/50] Batch 200/782                   Loss D: -0.8339, loss G: 0.2162\n",
      "Epoch [7/50] Batch 300/782                   Loss D: -0.7991, loss G: 0.5832\n",
      "Epoch [7/50] Batch 400/782                   Loss D: -0.7060, loss G: 0.1869\n",
      "Epoch [7/50] Batch 500/782                   Loss D: -0.7360, loss G: 0.2351\n",
      "Epoch [7/50] Batch 600/782                   Loss D: -0.8270, loss G: 0.4872\n",
      "Epoch [7/50] Batch 700/782                   Loss D: -0.7553, loss G: 0.2313\n",
      "Epoch [8/50] Batch 100/782                   Loss D: -0.8148, loss G: 0.4966\n",
      "Epoch [8/50] Batch 200/782                   Loss D: -0.6412, loss G: 0.0892\n",
      "Epoch [8/50] Batch 300/782                   Loss D: -0.7254, loss G: 0.5956\n",
      "Epoch [8/50] Batch 400/782                   Loss D: -0.6485, loss G: 0.5864\n",
      "Epoch [8/50] Batch 500/782                   Loss D: -0.7007, loss G: 0.2019\n",
      "Epoch [8/50] Batch 600/782                   Loss D: -0.7601, loss G: 0.5849\n",
      "Epoch [8/50] Batch 700/782                   Loss D: -0.7889, loss G: 0.2360\n",
      "Epoch [9/50] Batch 100/782                   Loss D: -0.5628, loss G: 0.0733\n",
      "Epoch [9/50] Batch 200/782                   Loss D: -0.7200, loss G: 0.2225\n",
      "Epoch [9/50] Batch 300/782                   Loss D: -0.6409, loss G: 0.1455\n",
      "Epoch [9/50] Batch 400/782                   Loss D: -0.6505, loss G: 0.1926\n",
      "Epoch [9/50] Batch 500/782                   Loss D: -0.7240, loss G: 0.2845\n",
      "Epoch [9/50] Batch 600/782                   Loss D: -0.6788, loss G: 0.5740\n",
      "Epoch [9/50] Batch 700/782                   Loss D: -0.6911, loss G: 0.1574\n",
      "Epoch [10/50] Batch 100/782                   Loss D: -0.5761, loss G: 0.1994\n",
      "Epoch [10/50] Batch 200/782                   Loss D: -0.6460, loss G: 0.1415\n",
      "Epoch [10/50] Batch 300/782                   Loss D: -0.6950, loss G: 0.5701\n",
      "Epoch [10/50] Batch 400/782                   Loss D: -0.7793, loss G: 0.5414\n",
      "Epoch [10/50] Batch 500/782                   Loss D: -0.6120, loss G: 0.1655\n",
      "Epoch [10/50] Batch 600/782                   Loss D: -0.6959, loss G: 0.5746\n",
      "Epoch [10/50] Batch 700/782                   Loss D: -0.6484, loss G: 0.4354\n",
      "Epoch [11/50] Batch 100/782                   Loss D: -0.5959, loss G: 0.4273\n",
      "Epoch [11/50] Batch 200/782                   Loss D: -0.6976, loss G: 0.5504\n",
      "Epoch [11/50] Batch 300/782                   Loss D: -0.6402, loss G: 0.1658\n",
      "Epoch [11/50] Batch 400/782                   Loss D: -0.6333, loss G: 0.4743\n",
      "Epoch [11/50] Batch 500/782                   Loss D: -0.6517, loss G: 0.1837\n",
      "Epoch [11/50] Batch 600/782                   Loss D: -0.6272, loss G: 0.5230\n",
      "Epoch [11/50] Batch 700/782                   Loss D: -0.5996, loss G: 0.1762\n",
      "Epoch [12/50] Batch 100/782                   Loss D: -0.6775, loss G: 0.5446\n",
      "Epoch [12/50] Batch 200/782                   Loss D: -0.6475, loss G: 0.1758\n",
      "Epoch [12/50] Batch 300/782                   Loss D: -0.6623, loss G: 0.2044\n",
      "Epoch [12/50] Batch 400/782                   Loss D: -0.6311, loss G: 0.2298\n",
      "Epoch [12/50] Batch 500/782                   Loss D: -0.5599, loss G: 0.5091\n",
      "Epoch [12/50] Batch 600/782                   Loss D: -0.6451, loss G: 0.1277\n",
      "Epoch [12/50] Batch 700/782                   Loss D: -0.6578, loss G: 0.4015\n",
      "Epoch [13/50] Batch 100/782                   Loss D: -0.7073, loss G: 0.2290\n",
      "Epoch [13/50] Batch 200/782                   Loss D: -0.7313, loss G: 0.3040\n",
      "Epoch [13/50] Batch 300/782                   Loss D: -0.6454, loss G: 0.1973\n",
      "Epoch [13/50] Batch 400/782                   Loss D: -0.6387, loss G: 0.1877\n",
      "Epoch [13/50] Batch 500/782                   Loss D: -0.6121, loss G: 0.5500\n",
      "Epoch [13/50] Batch 600/782                   Loss D: -0.5986, loss G: 0.1508\n",
      "Epoch [13/50] Batch 700/782                   Loss D: -0.5858, loss G: 0.5505\n",
      "Epoch [14/50] Batch 100/782                   Loss D: -0.6990, loss G: 0.2573\n",
      "Epoch [14/50] Batch 200/782                   Loss D: -0.6993, loss G: 0.2378\n",
      "Epoch [14/50] Batch 300/782                   Loss D: -0.6029, loss G: 0.5292\n",
      "Epoch [14/50] Batch 400/782                   Loss D: -0.7622, loss G: 0.2297\n",
      "Epoch [14/50] Batch 500/782                   Loss D: -0.7027, loss G: 0.2342\n",
      "Epoch [14/50] Batch 600/782                   Loss D: -0.6405, loss G: 0.1388\n",
      "Epoch [14/50] Batch 700/782                   Loss D: -0.6887, loss G: 0.4387\n",
      "Epoch [15/50] Batch 100/782                   Loss D: -0.6846, loss G: 0.5443\n",
      "Epoch [15/50] Batch 200/782                   Loss D: -0.5700, loss G: 0.1651\n",
      "Epoch [15/50] Batch 300/782                   Loss D: -0.5506, loss G: 0.1299\n",
      "Epoch [15/50] Batch 400/782                   Loss D: -0.6305, loss G: 0.5654\n",
      "Epoch [15/50] Batch 500/782                   Loss D: -0.6479, loss G: 0.5379\n",
      "Epoch [15/50] Batch 600/782                   Loss D: -0.6624, loss G: 0.1907\n",
      "Epoch [15/50] Batch 700/782                   Loss D: -0.6530, loss G: 0.1823\n",
      "Epoch [16/50] Batch 100/782                   Loss D: -0.7535, loss G: 0.5772\n",
      "Epoch [16/50] Batch 200/782                   Loss D: -0.6664, loss G: 0.5227\n",
      "Epoch [16/50] Batch 300/782                   Loss D: -0.6446, loss G: 0.1178\n",
      "Epoch [16/50] Batch 400/782                   Loss D: -0.6685, loss G: 0.5666\n",
      "Epoch [16/50] Batch 500/782                   Loss D: -0.6289, loss G: 0.5332\n",
      "Epoch [16/50] Batch 600/782                   Loss D: -0.8056, loss G: 0.5380\n",
      "Epoch [16/50] Batch 700/782                   Loss D: -0.7200, loss G: 0.5064\n",
      "Epoch [17/50] Batch 100/782                   Loss D: -0.6326, loss G: 0.5520\n",
      "Epoch [17/50] Batch 200/782                   Loss D: -0.5112, loss G: 0.0972\n",
      "Epoch [17/50] Batch 300/782                   Loss D: -0.6475, loss G: 0.1780\n",
      "Epoch [17/50] Batch 400/782                   Loss D: -0.6545, loss G: 0.2539\n",
      "Epoch [17/50] Batch 500/782                   Loss D: -0.7197, loss G: 0.5477\n",
      "Epoch [17/50] Batch 600/782                   Loss D: -0.6125, loss G: 0.2734\n",
      "Epoch [17/50] Batch 700/782                   Loss D: -0.6765, loss G: 0.5374\n",
      "Epoch [18/50] Batch 100/782                   Loss D: -0.7710, loss G: 0.5546\n",
      "Epoch [18/50] Batch 200/782                   Loss D: -0.6754, loss G: 0.5445\n",
      "Epoch [18/50] Batch 300/782                   Loss D: -0.6869, loss G: 0.2515\n",
      "Epoch [18/50] Batch 400/782                   Loss D: -0.6471, loss G: 0.1939\n",
      "Epoch [18/50] Batch 500/782                   Loss D: -0.6395, loss G: 0.5509\n",
      "Epoch [18/50] Batch 600/782                   Loss D: -0.6929, loss G: 0.4404\n",
      "Epoch [18/50] Batch 700/782                   Loss D: -0.7333, loss G: 0.1663\n",
      "Epoch [19/50] Batch 100/782                   Loss D: -0.5704, loss G: 0.5260\n",
      "Epoch [19/50] Batch 200/782                   Loss D: -0.7912, loss G: 0.5881\n",
      "Epoch [19/50] Batch 300/782                   Loss D: -0.5591, loss G: 0.2013\n",
      "Epoch [19/50] Batch 400/782                   Loss D: -0.7486, loss G: 0.5654\n",
      "Epoch [19/50] Batch 500/782                   Loss D: -0.7168, loss G: 0.5751\n",
      "Epoch [19/50] Batch 600/782                   Loss D: -0.7109, loss G: 0.1166\n",
      "Epoch [19/50] Batch 700/782                   Loss D: -0.8908, loss G: 0.5664\n",
      "Epoch [20/50] Batch 100/782                   Loss D: -0.6848, loss G: 0.3037\n",
      "Epoch [20/50] Batch 200/782                   Loss D: -0.7314, loss G: 0.0513\n",
      "Epoch [20/50] Batch 300/782                   Loss D: -0.6825, loss G: 0.5501\n",
      "Epoch [20/50] Batch 400/782                   Loss D: -0.7113, loss G: 0.5510\n",
      "Epoch [20/50] Batch 500/782                   Loss D: -0.7237, loss G: 0.4100\n",
      "Epoch [20/50] Batch 600/782                   Loss D: -0.6695, loss G: 0.3266\n",
      "Epoch [20/50] Batch 700/782                   Loss D: -0.7025, loss G: 0.1403\n",
      "Epoch [21/50] Batch 100/782                   Loss D: -0.5901, loss G: 0.0177\n",
      "Epoch [21/50] Batch 200/782                   Loss D: -0.3635, loss G: -0.0438\n",
      "Epoch [21/50] Batch 300/782                   Loss D: -0.6446, loss G: 0.5237\n",
      "Epoch [21/50] Batch 400/782                   Loss D: -0.6034, loss G: 0.5292\n",
      "Epoch [21/50] Batch 500/782                   Loss D: -0.6269, loss G: 0.5772\n",
      "Epoch [21/50] Batch 600/782                   Loss D: -0.7482, loss G: 0.0681\n",
      "Epoch [21/50] Batch 700/782                   Loss D: -0.9876, loss G: 0.5323\n",
      "Epoch [22/50] Batch 100/782                   Loss D: -0.5852, loss G: 0.2586\n",
      "Epoch [22/50] Batch 200/782                   Loss D: -0.6361, loss G: 0.4628\n",
      "Epoch [22/50] Batch 300/782                   Loss D: -0.5873, loss G: 0.2258\n",
      "Epoch [22/50] Batch 400/782                   Loss D: -0.7750, loss G: 0.5556\n",
      "Epoch [22/50] Batch 500/782                   Loss D: -0.8773, loss G: 0.5777\n",
      "Epoch [22/50] Batch 600/782                   Loss D: -0.7048, loss G: 0.1598\n",
      "Epoch [22/50] Batch 700/782                   Loss D: -0.4952, loss G: 0.5225\n",
      "Epoch [23/50] Batch 100/782                   Loss D: -0.6521, loss G: 0.1679\n",
      "Epoch [23/50] Batch 200/782                   Loss D: -0.5473, loss G: 0.1585\n",
      "Epoch [23/50] Batch 300/782                   Loss D: -0.5794, loss G: 0.1136\n",
      "Epoch [23/50] Batch 400/782                   Loss D: -0.6119, loss G: 0.1072\n",
      "Epoch [23/50] Batch 500/782                   Loss D: -0.6125, loss G: 0.2146\n",
      "Epoch [23/50] Batch 600/782                   Loss D: -0.7869, loss G: 0.5662\n",
      "Epoch [23/50] Batch 700/782                   Loss D: -0.7116, loss G: 0.1912\n",
      "Epoch [24/50] Batch 100/782                   Loss D: -0.6487, loss G: 0.1850\n",
      "Epoch [24/50] Batch 200/782                   Loss D: -0.9378, loss G: 0.5600\n",
      "Epoch [24/50] Batch 300/782                   Loss D: -0.5096, loss G: -0.0163\n",
      "Epoch [24/50] Batch 400/782                   Loss D: -0.6705, loss G: 0.2824\n",
      "Epoch [24/50] Batch 500/782                   Loss D: -0.5591, loss G: 0.4624\n",
      "Epoch [24/50] Batch 600/782                   Loss D: -0.7094, loss G: 0.2528\n",
      "Epoch [24/50] Batch 700/782                   Loss D: -0.6634, loss G: 0.5872\n",
      "Epoch [25/50] Batch 100/782                   Loss D: -0.6867, loss G: 0.2648\n",
      "Epoch [25/50] Batch 200/782                   Loss D: -0.5936, loss G: 0.5392\n",
      "Epoch [25/50] Batch 300/782                   Loss D: -0.4898, loss G: 0.1072\n",
      "Epoch [25/50] Batch 400/782                   Loss D: -0.5194, loss G: 0.2384\n",
      "Epoch [25/50] Batch 500/782                   Loss D: -0.7215, loss G: 0.5642\n",
      "Epoch [25/50] Batch 600/782                   Loss D: -0.5560, loss G: 0.2132\n",
      "Epoch [25/50] Batch 700/782                   Loss D: -0.6181, loss G: 0.5810\n",
      "Epoch [26/50] Batch 100/782                   Loss D: -0.5598, loss G: 0.0987\n",
      "Epoch [26/50] Batch 200/782                   Loss D: -0.5554, loss G: 0.1934\n",
      "Epoch [26/50] Batch 300/782                   Loss D: -0.6166, loss G: 0.0549\n",
      "Epoch [26/50] Batch 400/782                   Loss D: -0.7021, loss G: 0.2631\n",
      "Epoch [26/50] Batch 500/782                   Loss D: -0.5760, loss G: 0.1374\n",
      "Epoch [26/50] Batch 600/782                   Loss D: -0.7008, loss G: 0.5553\n",
      "Epoch [26/50] Batch 700/782                   Loss D: -0.6946, loss G: 0.5517\n",
      "Epoch [27/50] Batch 100/782                   Loss D: -0.5482, loss G: 0.5709\n",
      "Epoch [27/50] Batch 200/782                   Loss D: -0.6618, loss G: 0.2689\n",
      "Epoch [27/50] Batch 300/782                   Loss D: -0.6481, loss G: 0.1310\n",
      "Epoch [27/50] Batch 400/782                   Loss D: -0.6155, loss G: 0.0810\n",
      "Epoch [27/50] Batch 500/782                   Loss D: -0.4925, loss G: 0.5400\n",
      "Epoch [27/50] Batch 600/782                   Loss D: -0.6485, loss G: 0.1100\n",
      "Epoch [27/50] Batch 700/782                   Loss D: -0.5100, loss G: 0.5308\n",
      "Epoch [28/50] Batch 100/782                   Loss D: -0.7363, loss G: 0.5416\n",
      "Epoch [28/50] Batch 200/782                   Loss D: -0.8075, loss G: 0.5681\n",
      "Epoch [28/50] Batch 300/782                   Loss D: -0.7020, loss G: 0.0695\n",
      "Epoch [28/50] Batch 400/782                   Loss D: -0.6638, loss G: 0.2245\n",
      "Epoch [28/50] Batch 500/782                   Loss D: -0.6517, loss G: 0.1349\n",
      "Epoch [28/50] Batch 600/782                   Loss D: -0.5670, loss G: 0.1881\n",
      "Epoch [28/50] Batch 700/782                   Loss D: -0.5915, loss G: 0.5199\n",
      "Epoch [29/50] Batch 100/782                   Loss D: -0.5790, loss G: 0.5391\n",
      "Epoch [29/50] Batch 200/782                   Loss D: -0.6448, loss G: 0.1649\n",
      "Epoch [29/50] Batch 300/782                   Loss D: -0.5774, loss G: 0.2604\n",
      "Epoch [29/50] Batch 400/782                   Loss D: -0.6279, loss G: 0.5475\n",
      "Epoch [29/50] Batch 500/782                   Loss D: -0.9422, loss G: 0.5662\n",
      "Epoch [29/50] Batch 600/782                   Loss D: -0.4410, loss G: 0.0780\n",
      "Epoch [29/50] Batch 700/782                   Loss D: -0.6593, loss G: 0.0670\n",
      "Epoch [30/50] Batch 100/782                   Loss D: -0.7043, loss G: 0.5555\n",
      "Epoch [30/50] Batch 200/782                   Loss D: -0.4844, loss G: 0.4371\n",
      "Epoch [30/50] Batch 300/782                   Loss D: -0.5133, loss G: 0.5348\n",
      "Epoch [30/50] Batch 400/782                   Loss D: -0.5270, loss G: 0.5253\n",
      "Epoch [30/50] Batch 500/782                   Loss D: -0.7213, loss G: 0.5591\n",
      "Epoch [30/50] Batch 600/782                   Loss D: -0.5078, loss G: 0.0240\n",
      "Epoch [30/50] Batch 700/782                   Loss D: -0.6256, loss G: 0.5384\n",
      "Epoch [31/50] Batch 100/782                   Loss D: -0.6973, loss G: 0.5254\n",
      "Epoch [31/50] Batch 200/782                   Loss D: -0.6855, loss G: 0.0869\n",
      "Epoch [31/50] Batch 300/782                   Loss D: -0.4907, loss G: 0.5004\n",
      "Epoch [31/50] Batch 400/782                   Loss D: -0.5659, loss G: 0.0164\n",
      "Epoch [31/50] Batch 500/782                   Loss D: -0.5728, loss G: 0.0646\n",
      "Epoch [31/50] Batch 600/782                   Loss D: -0.5468, loss G: 0.0898\n",
      "Epoch [31/50] Batch 700/782                   Loss D: -0.6305, loss G: 0.2525\n",
      "Epoch [32/50] Batch 100/782                   Loss D: -0.6759, loss G: 0.0722\n",
      "Epoch [32/50] Batch 200/782                   Loss D: -0.5095, loss G: 0.1285\n",
      "Epoch [32/50] Batch 300/782                   Loss D: -0.7043, loss G: 0.1261\n",
      "Epoch [32/50] Batch 400/782                   Loss D: -0.6316, loss G: 0.4787\n",
      "Epoch [32/50] Batch 500/782                   Loss D: -0.5936, loss G: 0.1062\n",
      "Epoch [32/50] Batch 600/782                   Loss D: -0.3936, loss G: 0.4835\n",
      "Epoch [32/50] Batch 700/782                   Loss D: -0.4838, loss G: 0.5024\n",
      "Epoch [33/50] Batch 100/782                   Loss D: -0.6533, loss G: 0.5389\n",
      "Epoch [33/50] Batch 200/782                   Loss D: -0.5003, loss G: 0.1412\n",
      "Epoch [33/50] Batch 300/782                   Loss D: -0.5398, loss G: 0.5742\n",
      "Epoch [33/50] Batch 400/782                   Loss D: -0.4174, loss G: -0.0789\n"
     ]
    }
   ],
   "source": [
    "genet = Generator(z_dimension, image_channels, generator_features).to(device)\n",
    "critic = Discriminator(image_channels, discriminator_features).to(device)\n",
    "initialize_weights(genet)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# initializate optimizer\n",
    "opt_gen = optim.RMSprop(genet.parameters(), lr=learning_rate)\n",
    "opt_critic = optim.RMSprop(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(10, z_dimension, 1, 1).to(device)\n",
    "step = 0\n",
    "loss_ge = []\n",
    "loss_de = []\n",
    "genet.train()\n",
    "critic.train()\n",
    "fakeimg_list=[]\n",
    "img_list=[]\n",
    "for epoch in range(epochs):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        cur_batch_size = data.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        for _ in range(discriminator_iterations):\n",
    "            noise = torch.randn(cur_batch_size, z_dimension, 1, 1).to(device)\n",
    "            fake = genet(noise)\n",
    "            critic_real = critic(data).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            critic.zero_grad()\n",
    "            \n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "            # clip critic weights between -0.01, 0.01\n",
    "            for p in critic.parameters():\n",
    "                p.data.clamp_(-weight, weight)\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        \n",
    "        genet.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        loss_de.append(loss_critic.detach().cpu())\n",
    "        loss_ge.append(loss_gen.detach().cpu())\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            genet.eval()\n",
    "            critic.eval()\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fak = genet(fixed_noise).detach().cpu()\n",
    "                realI = data.detach().cpu()\n",
    "                fakeimg_list.append(torchvision.utils.make_grid(fak, padding=2, normalize=True))\n",
    "                img_list.append(torchvision.utils.make_grid(realI, padding=2, normalize=True))\n",
    "            step += 1\n",
    "            genet.train()\n",
    "            critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b2b24-6854-4953-92ec-22069ef4afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title('Noise Epoch 0')\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in fakeimg_list[0:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13623-01f6-42d9-a7ec-be3194817a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = fakeimg_list[0:30]\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title('Noise Epoch 30')\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1c19b-3db5-494f-b330-06da75111e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title('Final Epoch')\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in fakeimg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b185a7-9b5d-4ce5-ad70-d8c1e0b2b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Losses')\n",
    "plt.plot(loss_ge,label = \"generator loss\")\n",
    "#plt.plot(loss_de,label = \"descrimanator loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35057d9f-48d1-4774-aa58-f079d46325f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
