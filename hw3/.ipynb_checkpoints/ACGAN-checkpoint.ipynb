{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1184bbd3-c9b7-43ff-8620-738893d40885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73f75e1-2765-48ae-a3d1-d4d6d8c40e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device setup\n",
    "device = 'cpu'\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35e09d4-c304-4a72-8126-bd6b03d114ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#set up seed for consistancy\n",
    "torch.manual_seed(1)\n",
    "#hyperparameters based on slides\n",
    "learning_rate = 2e-4\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "#black and white 1 for color img 3(r.g.b)\n",
    "image_channels = 3\n",
    "noise_dimension = 100\n",
    "epochs = 10\n",
    "discriminator_features = 64\n",
    "generator_features = 64\n",
    "adam_beta = 0.5\n",
    "latent_dim = 100\n",
    "n_classes = 10\n",
    "#get Cifar10 dataset\n",
    "workers = 2\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(image_channels)], [0.5 for _ in range(image_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "data_train = torchvision.datasets.CIFAR10(root='./DL HW 3/data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms,  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                          shuffle=True,num_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6918129b-8cc7-4c17-b1b6-f0f76817add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discrim = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(1, 16, 3, 2, 1), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout2d(0.25),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._descrimStructure(16, 16 * 2, 4, 2, 1),\n",
    "            self._descrimStructure(16 * 2, 16 * 4, 4, 2, 1),\n",
    "            self._descrimStructure(16 * 4, 16 * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        ds_size = 64 // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def _descrimStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discrim(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim) \n",
    "        self.init_size = image_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "        self.genet = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._genetStructure(128, 128, 3, 1, 1),  # img: 4x4\n",
    "            self._genetStructure(128, 64, 3, 1, 1),  # img: 8x8\n",
    "            nn.Conv2d(64, 1, 3, 1, 1),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _genetStructure(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels,0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size,-1)\n",
    "        img = self.genet(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004eca9e-e422-4aeb-a516-84737d46c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34f9184-d401-4b26-9e24-58aa7a9a687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (label_emb): Embedding(10, 100)\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=32768, bias=True)\n",
      "  )\n",
      "  (genet): Sequential(\n",
      "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_genetStructure() missing 4 required positional arguments: 'out_channels', 'kernel_size', 'stride', and 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m gen_labels \u001b[38;5;241m=\u001b[39m Variable(LongTensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_classes, batch_size)))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(noise\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 32\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43mgenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgen_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[1;32m     35\u001b[0m disc_real \u001b[38;5;241m=\u001b[39m discrim(real)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise, labels)\u001b[0m\n\u001b[1;32m     75\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1(gen_input)\n\u001b[1;32m     76\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m128\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_size,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_genetStructure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenet(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: _genetStructure() missing 4 required positional arguments: 'out_channels', 'kernel_size', 'stride', and 'padding'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "genet = Generator(noise_dimension, image_channels, generator_features).to(device)\n",
    "discrim = Discriminator(image_channels, discriminator_features).to(device)\n",
    "#genet.summary()\n",
    "#discrim.summary()\n",
    "print(genet)\n",
    "model_weights(genet)\n",
    "model_weights(discrim)\n",
    "gen_loss = []\n",
    "disc_loss = []\n",
    "opt_gen = optim.Adam(genet.parameters(), lr=learning_rate, betas=(0.5,0.999))\n",
    "opt_disc = optim.Adam(discrim.parameters(), lr=learning_rate, betas=(0.5,0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, noise_dimension, 1, 1).to(device)\n",
    "step = 0\n",
    "\n",
    "genet.train()\n",
    "discrim.train()\n",
    "fakeimg_list=[]\n",
    "img_list=[]\n",
    "for epoch in range(epochs):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(train_loader):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(batch_size, noise_dimension, 1, 1).to(device)\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "        print(noise.shape)\n",
    "        fake = genet(noise,gen_labels)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = discrim(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = discrim(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        discrim.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = discrim(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        genet.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        gen_loss.append(loss_gen)\n",
    "        disc_loss.append(loss_disc)\n",
    "        \n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fak = genet(fixed_noise).detach().cpu()\n",
    "                realI = real.detach().cpu()\n",
    "                fakeimg_list.append(torchvision.utils.make_grid(fak, padding=2, normalize=True))\n",
    "                img_list.append(torchvision.utils.make_grid(realI, padding=2, normalize=True))\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca3777-9f22-4f0b-8c50-3965f63eac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a59101-04d7-4a19-8736-7de500bbc732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
